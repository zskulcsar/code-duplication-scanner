#!/usr/bin/env python3
"""Analyze CSV records for duplicated md5 values and similar intents.

# TODO: the below needs adjusting, so that:
  - dosctring is ignored
  - line numbers added for classes and functions
  - md5sum works only on the code; ie.: comment lines ignored
  - class and method includes file_path
Iterate over every source code file and do the following:
- `file_path` - The path of the file relative to the project's root
- `file_md5` - md5 of the file's content
- `file_intent` - Short description of the intent expressed by the code in the file
- for every class:
  - `class_signature` - the signature of the class
  - `class_md5` - the md5 hash of the source code within the class
  - `class_intent` - Short description of the intent expressed by the code in the file
- for every function and method:
  - `fun_signature` - the signature of the method or function
  - `fun_md5` - the md5 hash of the method of function body
  - `fun_intent` - Short description of the intent expressed by the code in the file

For every piece of code, the comment lines must be stripped and `md5sum` must be calculated on the bare code lines.
The `intent` is generated by the LLM based on the bare code lines to prevent the intent in the documentation misguding the intent generation.
Line numbers should record the original start and end positions in the file including code lines for accurate identification.

For every file, the above should be reppresented as a reports as follows:

| Signature | md5sum | Intent |
|-----------|--------|--------|
| one of `file_path`, `class_signature`, `fun_signature` | one of `file_md5`, `class_md5`, `fun_md5` | one of `file_intent`, `class_intent`, `fun_intent` |

# TODO: for the analysis part (this file) we should have an extra flag telling wheter code, or test.
  - perhaps, better to do filtering on `file_path` match.

# TODO: perhaps the whole thing should be done in 3 steps:
  - code that splits the codebase into a strucuture (ie.: actually simple code processing, no AI - no nothing)
  - the enrichment parts calculates md5 & intent usign a simple model to generate intent (ie.: this ensures isolation from the agent on the intent part)
  - analysis (like now, but more user firendly)

`codex resume 019c7a63-d3cb-7962-9d7e-524eb6e61c21`
"""

import argparse
import csv
import logging
from dataclasses import dataclass
from pathlib import Path
import sys

import Levenshtein
from rich.logging import RichHandler


logger = logging.getLogger(__name__)


@dataclass(frozen=True)
class Record:
    """Represent one report record loaded from CSV."""

    signature: str
    md5sum: str
    intent: str
    filename: str


@dataclass(frozen=True)
class FileSection:
    """Represent one file section and all rows that belong to it."""

    filename: str
    records: list[Record]


def configure_logging(level: int = logging.INFO) -> None:
    """Configure logging with Rich handler.

    Args:
        level: Logging severity threshold.
    """
    logging.basicConfig(
        level=level,
        format="%(message)s",
        handlers=[RichHandler(rich_tracebacks=True, show_path=False)],
    )


def parse_args(argv: list[str] | None = None) -> argparse.Namespace:
    """Parse CLI arguments.

    Args:
        argv: Optional argument vector override.

    Returns:
        Parsed arguments namespace.
    """
    parser = argparse.ArgumentParser(prog="csv-md5-intent-check")
    parser.add_argument("fun", help="The function to call")
    parser.add_argument("csv_path", help="Path to CSV file to analyze.")
    parser.add_argument(
        "threshold",
        type=float,
        help="Levenshtein ratio threshold in the [0.0, 1.0] range.",
    )
    return parser.parse_args(argv)


def load_records(csv_path: Path) -> list[Record]:
    """Load record rows from a CSV file.

    Args:
        csv_path: CSV file path.

    Returns:
        Parsed records in file order.

    Raises:
        ValueError: If required columns are missing.
    """
    sections = load_sections(csv_path)
    return [record for section in sections for record in section.records]


def normalize_signature(signature: str) -> str:
    """Normalize signature text for matching control rows.

    Args:
        signature: Raw signature string from CSV.

    Returns:
        Signature without surrounding backticks/quotes and extra whitespace.
    """
    value = signature.strip()
    if len(value) >= 2 and value[0] == value[-1] and value[0] in {"`", '"', "'"}:
        value = value[1:-1].strip()
    return value


def load_sections(csv_path: Path) -> list[FileSection]:
    """Load CSV and assign rows to file sections.

    The section starts when a `Signature` value begins with `file_path:` and
    continues until the next `file_path:` row.

    Args:
        csv_path: CSV file path.

    Returns:
        Parsed file sections with filename-aware records.

    Raises:
        ValueError: If required columns are missing.
    """
    with csv_path.open("r", encoding="utf-8", newline="") as handle:
        reader = csv.DictReader(handle)
        headers = set(reader.fieldnames or [])
        required = {"Signature", "md5sum", "Intent"}
        if not required.issubset(headers):
            raise ValueError("CSV must contain headers: Signature, md5sum, Intent.")

        sections: list[FileSection] = []
        current_filename = ""
        current_records: list[Record] = []

        for row in reader:
            signature = (row.get("Signature") or "").strip()
            normalized_signature = normalize_signature(signature)

            if normalized_signature.startswith("file_path:"):
                if current_records:
                    sections.append(
                        FileSection(filename=current_filename, records=current_records)
                    )
                    current_records = []
                current_filename = normalized_signature.removeprefix(
                    "file_path:"
                ).strip()

            current_records.append(
                Record(
                    signature=signature,
                    md5sum=(row.get("md5sum") or "").strip(),
                    intent=(row.get("Intent") or "").strip(),
                    filename=current_filename,
                )
            )

        if current_records:
            sections.append(
                FileSection(filename=current_filename, records=current_records)
            )

    return sections


def iter_md5_duplicates(records: list[Record]) -> dict[str, list[Record]]:
    """Group records with duplicate md5 values.

    Args:
        records: Input records.

    Returns:
        Mapping for md5 values with more than one record.
    """
    grouped: dict[str, list[Record]] = {}
    for record in records:
        grouped.setdefault(record.md5sum, []).append(record)
    return {md5: group for md5, group in grouped.items() if len(group) > 1}


def iter_similarity_pairs(
    records: list[Record], threshold: float
) -> list[tuple[float, Record, Record]]:
    """Return record intent pairs meeting the Levenshtein ratio threshold.

    Args:
        records: Input records.
        threshold: Minimum ratio to include, in [0.0, 1.0].

    Returns:
        Matched record pairs with their ratio.
    """
    matches: list[tuple[float, Record, Record]] = []
    for i, left in enumerate(records):
        for right in records[i + 1 :]:
            score = Levenshtein.ratio(left.intent, right.intent)
            if score >= threshold:
                matches.append((score, left, right))
    return matches


def clean_output_value(value: str) -> str:
    """Normalize output value formatting.

    Args:
        value: Raw value.

    Returns:
        Value with backticks discarded and whitespace trimmed.
    """
    return value.replace("`", "").strip()


def write_md5_csv(
    duplicates: dict[str, list[Record]],
    output_path: Path,
) -> None:
    """Write duplicate md5 matches as quoted CSV rows.

    Args:
        duplicates: Grouped duplicate records by md5.
        output_path: Target CSV file path.
    """
    with output_path.open("w", encoding="utf-8", newline="") as handle:
        writer = csv.writer(handle, quoting=csv.QUOTE_ALL, lineterminator="\n")
        writer.writerow(["filename", "signature", "intent", "md5sum"])
        for _, rows in sorted(duplicates.items()):
            for row in rows:
                writer.writerow(
                    [
                        clean_output_value(row.filename),
                        clean_output_value(row.signature),
                        clean_output_value(row.intent),
                        clean_output_value(row.md5sum),
                    ]
                )


def write_levenshtein_csv(
    matches: list[tuple[float, Record, Record]],
    output_path: Path,
) -> None:
    """Write Levenshtein similarity matches as quoted CSV rows.

    Args:
        matches: Similarity match tuples.
        output_path: Target CSV file path.
    """
    with output_path.open("w", encoding="utf-8", newline="") as handle:
        writer = csv.writer(handle, quoting=csv.QUOTE_ALL, lineterminator="\n")
        writer.writerow(
            [
                "score",
                "filename",
                "signature",
                "intent",
                "md5sum",
                "other_filename",
                "other_signature",
                "other_intent",
                "other_md5sum",
            ]
        )
        for score, left, right in sorted(
            matches, key=lambda item: item[0], reverse=True
        ):
            writer.writerow(
                [
                    f"{score:.6f}",
                    clean_output_value(left.filename),
                    clean_output_value(left.signature),
                    clean_output_value(left.intent),
                    clean_output_value(left.md5sum),
                    clean_output_value(right.filename),
                    clean_output_value(right.signature),
                    clean_output_value(right.intent),
                    clean_output_value(right.md5sum),
                ]
            )


def analyze(csv_path: Path, threshold: float) -> int:
    """Run duplicate and similarity analysis for one CSV file.

    Args:
        csv_path: CSV file path.
        threshold: Similarity threshold in [0.0, 1.0].

    Returns:
        Process exit code.
    """
    if not csv_path.exists():
        logger.warning("CSV file does not exist: %s", csv_path)
        return 2
    if threshold < 0.0 or threshold > 1.0:
        logger.warning("Threshold must be within [0.0, 1.0], got: %s", threshold)
        return 2

    try:
        records = load_records(csv_path)
    except (OSError, csv.Error, ValueError) as exc:
        logger.warning("Failed to load CSV '%s': %s", csv_path, exc)
        return 2

    duplicates = iter_md5_duplicates(records)
    matches = iter_similarity_pairs(records, threshold)
    md5_output = csv_path.with_name(f"{csv_path.stem}.md5_matches.csv")
    levenshtein_output = csv_path.with_name(f"{csv_path.stem}.levenshtein_matches.csv")
    write_md5_csv(duplicates, md5_output)
    write_levenshtein_csv(matches, levenshtein_output)
    print(str(md5_output))
    print(str(levenshtein_output))
    return 0


def main(argv: list[str] | None = None) -> None:
    """Execute CLI and exit with process status code.

    Args:
        argv: Optional argument vector override.
    """
    configure_logging()
    args = parse_args(argv)

    match args.fun:
        case "analyze":
            exit_code = analyze(Path(args.csv_path), args.threshold)
        case _:
            logger.warning("Unsupported function: %s", args.fun)
            exit_code = 2

    raise SystemExit(exit_code)


if __name__ == "__main__":
    main(sys.argv[1:])
